{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rendu_Scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Scraping"
      ],
      "metadata": {
        "id": "IOxV1MCPoGAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importation des packages\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "#Définition des headers afin d'éviter de voir son accès au site bloqué lors du scraping\n",
        "\n",
        "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}"
      ],
      "metadata": {
        "id": "mosIytmMsHoH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stock symbol extraction and statistics scraping\n",
        "  (csv & xlsx files as output)"
      ],
      "metadata": {
        "id": "4Flk6SWbjmkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stock Symbol import\n",
        "\n",
        "#Chaque page contient une liste déroulante de 100 entreprises\n",
        "#L_ext contient l'index de départ pour chacun des onglets déroulants\n",
        "\n",
        "L_ext = [\"0\",\"100\",\"200\",\"300\",\"400\",\"500\"]\n",
        "stockDict3 = {}\n",
        "\n",
        "#On scrape tous les \"stocksymbol\" des entreprises que l'on veut étudier\n",
        "\n",
        "\n",
        "for x in L_ext :\n",
        "  url = \"https://finance.yahoo.com/screener/predefined/ms_technology?count=100&offset=\"+x\n",
        "  page = requests.get(url, headers=headers, timeout=5).text \n",
        "  soup = BeautifulSoup(page, 'html.parser') #On passe les données collectées en structure HTML\n",
        "  stock_data3 = soup.find_all('table')\n",
        "  for table in stock_data3: #On parcourt l'ensemble des tableaux de la page\n",
        "    trs = table.find_all('tr')\n",
        "    for tr in trs: #On parcourt l'ensemble des lignes du tableau\n",
        "        tds = tr.find_all('td')\n",
        "        if len(tds) > 0: #Si la ligne contient une valeur, on ajoute le stocksymbol à la liste\n",
        "            stockDict3[tds[0].get_text()] = [tds[1].get_text()]\n",
        "  \n",
        "stock_sum_df3 = pd.DataFrame(data=stockDict3)\n",
        "L_st_symb = list(stock_sum_df3) #On stocke l'ensemble des stocksymbol dans L_st_symb\n",
        "print(L_st_symb)\n",
        "len(L_st_symb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM2UWfkLfjGK",
        "outputId": "6fa8e131-579b-408d-aafd-e131c69ffab3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AAPL', 'MSFT', 'NVDA', 'TSM', 'ASML', 'AVGO', 'CSCO', 'ADBE', 'ACN', 'ORCL', 'CRM', 'INTC', 'QCOM', 'TXN', 'INTU', 'AMD', 'SAP', 'SONY', 'SHOP', 'AMAT', 'IBM', 'NOW', 'MU', 'INFY', 'LRCX', 'ADI', 'SNOW', 'UBER', 'TEAM', 'MRVL', 'FISV', 'FIS', 'SQ', 'KLAC', 'WDAY', 'COIN', 'NXPI', 'ADSK', 'SNPS', 'TEL', 'PANW', 'ZM', 'WIT', 'FTNT', 'APH', 'XLNX', 'VMW', 'CDNS', 'MCHP', 'CTSH', 'DDOG', 'STM', 'DELL', 'MSI', 'CRWD', 'HPQ', 'ANET', 'NET', 'ZS', 'ERIC', 'KEYS', 'U', 'NOK', 'PLTR', 'EPAM', 'GFS', 'GLW', 'ANSS', 'OKTA', 'APP', 'ZBRA', 'ON', 'UMC', 'DOCU', 'CDW', 'VRSN', 'TER', 'SWKS', 'FTV', 'STX', 'GRMN', 'CAJ', 'IT', 'HUBS', 'AFRM', 'HPE', 'ZI', 'MPWR', 'PAYC', 'SSNC', 'BILL', 'PATH', 'NTAP', 'GIB', 'TRMB', 'BR', 'TYL', 'WDC', 'TDY', 'FLT', 'UI', 'ENTG', 'SPLK', 'AKAM', 'NICE', 'NUAN', 'QRVO', 'CFLT', 'ASX', 'XM', 'RNG', 'CLVT', 'NLOK', 'DT', 'CHKP', 'LYFT', 'FFIV', 'TOST', 'HCP', 'PTC', 'CDAY', 'LOGI', 'WOLF', 'SEDG', 'HOOD', 'CGNX', 'GDDY', 'LDOS', 'JKHY', 'BSY', 'OTEX', 'BKI', 'FICO', 'ZEN', 'CTXS', 'PCTY', 'S', 'CIEN', 'IOT', 'ASAN', 'JNPR', 'SNX', 'GLOB', 'COUP', 'AUR', 'MNDY', 'ST', 'SYNA', 'GTLB', 'JBL', 'ESTC', 'AZPN', 'G', 'LSCC', 'MKSI', 'PCOR', 'AVLR', 'ARW', 'DOX', 'DLB', 'CNXC', 'DBX', 'GWRE', 'FSLR', 'MANH', 'IPGP', 'PSTG', 'NVEI', 'YMM', 'SMAR', 'DNB', 'DAVA', 'DXC', 'INFA', 'DLO', 'FLEX', 'WIX', 'MQ', 'PEGA', 'TIXT', 'SLAB', 'PAGS', 'OLED', 'LFUS', 'IIVI', 'LITE', 'DOCN', 'TWKS', 'LPL', 'IS', 'WEX', 'NTNX', 'NEWR', 'PLAN', 'COHR', 'DSGX', 'RUN', 'EEFT', 'ASGN', 'SWCH', 'CACI', 'CCCS', 'CYBR', 'ALGM', 'AMBA', 'AMKR', 'NOVT', 'BMBL', 'FRSH', 'WK', 'BRZE', 'BL', 'MSTR', 'NATI', 'NCR', 'SMTC', 'LSPD', 'ALTR', 'CCMP', 'LAZR', 'TENB', 'MXL', 'POWI', 'MIME', 'VICR', 'VNT', 'XRX', 'MTSI', 'CRCT', 'CRUS', 'ROG', 'BB', 'NCNO', 'CDK', 'ATC', 'QLYS', 'SAIC', 'STNE', 'SRAD', 'ONTO', 'DIOD', 'VRNS', 'MCFE', 'FOUR', 'TDC', 'TASK', 'ALIT', 'DV', 'EXLS', 'SPSC', 'PYCR', 'FN', 'TSEM', 'QTWO', 'ENV', 'WNS', 'TTEC', 'CALX', 'SAIL', 'AVT', 'VIAV', 'JAMF', 'ACIW', 'MSP', 'ALRM', 'KD', 'MNDT', 'AYX', 'NVMI', 'KLIC', 'PRFT', 'NSIT', 'DUOL', 'CXM', 'BOX', 'FSLY', 'DCT', 'SQSP', 'BLKB', 'KNBE', 'SONO', 'FORM', 'EVTC', 'CWAN', 'FLYW', 'VZIO', 'VRNT', 'VSAT', 'PAY', 'COMP', 'SPWR', 'ESMT', 'SIMO', 'VSH', 'RAMP', 'AI', 'CVLT', 'RMBS', 'KC', 'CRNC', 'ETWO', 'SABR', 'ITRI', 'MANT', 'DQ', 'MNTV', 'OLO', 'PWSC', 'IONQ', 'BDC', 'RXT', 'FROG', 'VCRA', 'EVCM', 'ACVA', 'PD', 'EXFY', 'SANM', 'PLXS', 'NOVA', 'RELY', 'AVDX', 'VMEO', 'UCTT', 'EPAY', 'DDD', 'ACLS', 'SEMR', 'LPSN', 'NTCT', 'WBX', 'STEM', 'SMCI', 'ESE', 'COMM', 'SATS', 'SWI', 'KN', 'MAXR', 'ZUO', 'HIMX', 'JKS', 'PRGS', 'STER', 'PI', 'XPER', 'CD', 'EXTR', 'INFN', 'YOU', 'CYXT', 'CSGS', 'CRSR', 'EGHT', 'MFGP', 'FORTY', 'NABL', 'SPNS', 'COHU', 'MEI', 'DCBO', 'VCSA', 'CSIQ', 'PING', 'LAW', 'RDWR', 'OSIS', 'API', 'GPRO', 'AVYA', 'TTMI', 'MLAB', 'EB', 'BAND', 'VTEX', 'FORG', 'SMRT', 'ZETA', 'SGH', 'ALKT', 'BLND', 'SSYS', 'AOSL', 'VECO', 'WKME', 'MLNK', 'PLUS', 'CINT', 'PRO', 'CLS', 'INTA', 'UIS', 'SUMO', 'SNPO', 'CLBT', 'PAR', 'SCWX', 'CNDT', 'ICHR', 'MTLS', 'IMOS', 'ENFN', 'POLY', 'VNET', 'FARO', 'VLD', 'GB', 'ATEN', 'CTS', 'HLIT', 'PLAB', 'YEXT', 'USER', 'AVPT', 'BTRS', 'CCSI', 'PDFS', 'RSKD', 'AUDC', 'CNXN', 'LASR', 'GSKY', 'AGYS', 'MKFG', 'MODN', 'MGIC', 'ADTN', 'CGNT', 'BHE', 'EBIX', 'CEVA', 'MX', 'LTCH', 'BASE', 'SCSC', 'NTGR', 'YALA', 'DGII', 'AMSWA', 'NPTN', 'INST', 'ONTF', 'LYLT', 'DBD', 'WEAV', 'BBAI', 'VLN', 'ECOM', 'SWIR', 'DMRC', 'HCKT', 'CMTL', 'CTLP', 'WEJO', 'MAXN', 'INSG', 'ITRN', 'IIIV', 'UEIC', 'QIWI', 'VPG', 'CASA', 'GILT', 'ALLT', 'ABST', 'SOL', 'BCOV', 'BTCM', 'AXTI', 'TUFN', 'AVNW', 'BKKT', 'TSAT', 'MAPS', 'SILC', 'CCRD', 'SANG', 'FATH', 'ZEPP', 'UEPS', 'MIXT', 'MYNA', 'CAMP', 'VOXX', 'DAKT', 'VIAO', 'RELL', 'SQNS', 'LYTS', 'KVHI', 'IMMR', 'BELFA', 'BELFB', 'CTG', 'ASYS', 'DSP', 'PCTI', 'TESS', 'MINDP', 'LVOXU', 'SABRP', 'CLVT-PA', 'AVGOP']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "503"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for x in L_st_symb :\n",
        "  url = 'https://finance.yahoo.com/quote/'+ stockSymbol + '/key-statistics?p=' + stockSymbol\n",
        "  page = requests.get(url, headers=headers, timeout=5).text \n",
        "  soup = BeautifulSoup(page, 'html.parser')\n",
        "  stock_data = soup.find_all('table')\n",
        "  #print(stock_data[0])\n",
        "  stockDict = {}\n",
        "\n",
        "for table in stock_data:\n",
        "    trs = table.find_all('tr')\n",
        "    for tr in trs:\n",
        "        tds = tr.find_all('td')\n",
        "        if len(tds) > 0:\n",
        "            stockDict[tds[0].get_text()] = [tds[1].get_text()]\n",
        "\n",
        "stock_sum_df = pd.DataFrame(data=stockDict)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q0zpVQDNj4CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "k = 0 #Le scraping étant long, k est un compteur indiquant l'avancée de l'éxécution du script (k_final = 510)\n",
        "L=[]\n",
        "\n",
        "for stockSymbol in L_st_symb : \n",
        "  url = 'https://finance.yahoo.com/quote/'+ stockSymbol + '/key-statistics?p=' + stockSymbol   #On parcourt l'onglet \"statistiques\" de chacune des entreprises dont on a récolté le stocksymbol\n",
        "  page = requests.get(url, headers=headers, timeout=120).text #Un timeout élevé a été sélectionné pour ne pas bloquer l'éxécution si la réponse du site prend du temps\n",
        "  soup = BeautifulSoup(page, 'html.parser')\n",
        "  stock_data = soup.find_all('table')\n",
        "  stockDict = {}\n",
        "  list_df = []\n",
        "    \n",
        "\n",
        "  for table in stock_data:\n",
        "    trs = table.find_all('tr')\n",
        "    for tr in trs:\n",
        "      tds = tr.find_all('td')\n",
        "      if len(tds) > 0:\n",
        "        stockDict[tds[0].get_text()] = [tds[1].get_text()]\n",
        "  k+=1 #On incrémente à chaque entreprise \n",
        "  L.append(pd.DataFrame(data=stockDict)) #L est une liste contenant les 510 entrées qui vont constituer le dataframe final\n",
        "  print(k)\n",
        "\n",
        "len(L)"
      ],
      "metadata": {
        "id": "SG_kNo9iuC3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#On concatène l'ensemble des entrées scrapées\n",
        "final_df = pd.concat(L)\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "vuPuVl-xIrzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#On définit l'index du dataframe par les stocksymbol collectés\n",
        "new_df = final_df.set_axis(L_st_symb, axis=0,inplace=False)\n",
        "new_df"
      ],
      "metadata": {
        "id": "e-wwSndVKM_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Le scraping étant long et dans l'esprit de fixer une base de données fixe pour l'étude, on convertit les données collectées en format csv et xlsx\n",
        "\n",
        "new_df.to_csv('donnees_python.csv') \n",
        "files.download('donnees_python.csv')\n",
        "new_df.to_excel('donnees_python.xlsx') \n",
        "files.download('donnees_python.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uFgMgO1BRtMD",
        "outputId": "0e806938-e9ac-45bd-fb02-85388614998f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_11c6121b-9c6f-4768-9929-128036de3d54\", \"donnees_python.csv\", 222645)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}